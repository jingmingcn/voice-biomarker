{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9d5f7-68c2-4821-88d7-21850c827e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version is:\", tf.__version__)\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "import seaborn as sns   # plotting heatmap\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882149d8-4834-4ccf-90d7-55cf890bf85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_df = pd.read_csv('/Users/jingming/DataSets/COVAREP/y_Train.tsv',sep='\\t',header=None)\n",
    "y_train = y_train_df.iloc[:,1]\n",
    "\n",
    "y_test_df = pd.read_csv('/Users/jingming/DataSets/COVAREP/y_Test.tsv',sep='\\t',header=None)\n",
    "y_test = y_test_df.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e9c00-1f90-4f8d-86f4-bfa2bf2f7779",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = []\n",
    "X_test_df = []\n",
    "for i in y_train_df.index:\n",
    "    id = y_train_df[0][i]\n",
    "    df = pd.read_csv((f'/Users/jingming/DataSets/COVAREP/{id}_COVAREP.csv'),sep=',',header=None)\n",
    "    X_train_df.append(df)\n",
    "    \n",
    "for i in y_test_df.index:\n",
    "    id = y_test_df[0][i]\n",
    "    df = pd.read_csv((f'/Users/jingming/DataSets/COVAREP/{id}_COVAREP.csv'),sep=',',header=None)\n",
    "    X_test_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53782926-7528-4577-92da-89490470259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 40000\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in y_train_df.index:\n",
    "    id = y_train_df[0][i]\n",
    "    df = X_train_df[i]\n",
    "    # df = df[df[1]==1]\n",
    "    # df = df.drop([ 1],axis=1)\n",
    "    n = df.to_numpy().shape[0]\n",
    "    if int(n/window_size) == 0:\n",
    "        print(i)\n",
    "    if y_train_df[1][i] == 1:\n",
    "        for j in range(0,int(n/window_size)):\n",
    "            # df = np.transpose(df.to_numpy())\n",
    "            # X_train.append(np.transpose(df[j*window_size:(j+1)*window_size].to_numpy()))\n",
    "            X_train.append(df[j*window_size:(j+1)*window_size].to_numpy())\n",
    "            y_train.append(y_train_df[1][i])\n",
    "\n",
    "    if y_train_df[1][i] == 0:\n",
    "        pad = int((n-window_size)/2)\n",
    "        \n",
    "        X_train.append(df[pad:(pad+window_size)].to_numpy())\n",
    "        y_train.append(y_train_df[1][i])\n",
    "    \n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "        \n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in y_test_df.index:\n",
    "    id = y_test_df[0][i]\n",
    "    # df = pd.read_csv((f'/Users/jingming/DataSets/COVAREP/{id}_COVAREP.csv'),sep=',',header=None)\n",
    "    df = X_test_df[i]\n",
    "    # df = df[df[1]==1]\n",
    "    # df = df.drop([ 1],axis=1)\n",
    "    n = df.to_numpy().shape[0]\n",
    "\n",
    "    if y_test_df[1][i] == 1:\n",
    "        for j in range(0,int(n/window_size)):\n",
    "            # df = np.transpose(df.to_numpy())\n",
    "            # X_test.append(np.transpose(df[j*window_size:(j+1)*window_size].to_numpy()))\n",
    "            X_test.append(df[j*window_size:(j+1)*window_size].to_numpy())\n",
    "            y_test.append(y_test_df[1][i])\n",
    "\n",
    "    if y_test_df[1][i] == 0:\n",
    "        pad = int((n-window_size)/2)\n",
    "        \n",
    "        X_test.append(df[pad:(pad+window_size)].to_numpy())\n",
    "        y_test.append(y_test_df[1][i])\n",
    "    \n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cdfb4e-0ba4-4a2a-b74d-c25a5ae73bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X_train , X_test))\n",
    "y = np.concatenate((y_train , y_test))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e612ab56-603d-4cf2-9c34-2c38812b093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from time import time\n",
    "\n",
    "print(f'window_size == {window_size}')\n",
    "\n",
    "# Let's make a list of CONSTANTS for modelling:\n",
    "LAYERS = [8, 8, 8, 1]                # number of units in hidden and output layers\n",
    "M_TRAIN = X_train.shape[0]           # number of training examples (2D)\n",
    "M_TEST = X_test.shape[0]             # number of test examples (2D),full=X_test.shape[0]\n",
    "N = X_train.shape[2]                 # number of features\n",
    "BATCH = M_TRAIN                          # batch size\n",
    "EPOCH = 50                           # number of epochs\n",
    "LR = 5e-2                            # learning rate of the gradient descent\n",
    "LAMBD = 3e-2                         # lambda in L2 regularizaion\n",
    "DP = 0.0                             # dropout rate\n",
    "RDP = 0.0                            # recurrent dropout rate\n",
    "T = window_size\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n",
    "\n",
    "\n",
    "# Build the Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_shape=(T, N), units=LAYERS[0],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "               dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(units=LAYERS[1],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "               dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(units=LAYERS[2],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "               dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=False, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=LAYERS[3], activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer=Adam(learning_rate=LR))\n",
    "print(model.summary())\n",
    "\n",
    "# Define a learning rate decay method:\n",
    "lr_decay = ReduceLROnPlateau(monitor='loss', \n",
    "                             patience=1, verbose=0, \n",
    "                             factor=0.5, min_lr=1e-8)\n",
    "# Define Early Stopping:\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                           patience=30, verbose=1, mode='auto',\n",
    "                           baseline=0, restore_best_weights=True)\n",
    "# Train the model. \n",
    "# The dataset is small for NN - let's use test_data for validation\n",
    "start = time()\n",
    "History = model.fit(X_train, y_train,\n",
    "                    epochs=EPOCH,\n",
    "                    batch_size=BATCH,\n",
    "                    validation_split=0.0,\n",
    "                    validation_data=(X_test[:M_TEST], y_test[:M_TEST]),\n",
    "                    shuffle=True,verbose=0,\n",
    "                    callbacks=[lr_decay, early_stop])\n",
    "print('-'*65)\n",
    "print(f'Training was completed in {time() - start:.2f} secs')\n",
    "print('-'*65)\n",
    "# Evaluate the model:\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train,\n",
    "                                       batch_size=M_TRAIN, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(X_test[:M_TEST], y_test[:M_TEST],\n",
    "                                     batch_size=M_TEST, verbose=0)\n",
    "print('-'*65)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "print(f'test error = {round((1 - test_acc) * M_TEST)} out of {M_TEST} examples')\n",
    "\n",
    "# Plot the loss and accuracy curves over epochs:\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,6))\n",
    "axs[0].plot(History.history['loss'], color='b', label='Training loss')\n",
    "axs[0].plot(History.history['val_loss'], color='r', label='Validation loss')\n",
    "axs[0].set_title(\"Loss curves\")\n",
    "axs[0].legend(loc='best', shadow=True)\n",
    "axs[1].plot(History.history['accuracy'], color='b', label='Training accuracy')\n",
    "axs[1].plot(History.history['val_accuracy'], color='r', label='Validation accuracy')\n",
    "axs[1].set_title(\"Accuracy curves\")\n",
    "axs[1].legend(loc='best', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9ecd0-d770-4967-86ba-5bd075ca70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "History.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff63244-9577-4654-b87f-a37fa8c7edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test, batch_size=BATCH, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(f'f1_score = ',f1_score(y_test, y_pred_bool))\n",
    "\n",
    "print(classification_report(y_test, y_pred_bool))\n",
    "\n",
    "confusion_matrix(y_test,y_pred_bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed446095-17e1-42ed-b68c-936fe29c488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc4eba-5e85-4ffa-87de-e20ff5d90e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0355dcb-2577-4c2b-8f1e-a1b9f34f2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'count of y_train == 0: {np.count_nonzero(y_train == 0)} \\ncount of y_test == 0: {np.count_nonzero(y_test == 0)} \\ncount of y_train == 1: {np.count_nonzero(y_train == 1)} \\ncount of y_test == 0: {np.count_nonzero(y_test == 1)}',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80be68-6e03-4785-840e-3bf23876bd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:voice-biomarker-py311]",
   "language": "python",
   "name": "conda-env-voice-biomarker-py311-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
